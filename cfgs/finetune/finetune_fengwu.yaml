data: 
  data_path : /dev/shm/store/checkpoint/original_dataset
  npy_name: /dev/shm/store/original_dataset/dataset.npy
  # buffer_folder: /mnt/vepfs/devel/weather/finetune-buffer
  large_buffer_folder: /mnt/vepfs/devel/weather/finetune-buffer
  buffer_folder: /dev/shm/store/buffer
  buffer_size: 2000
  epoch_size: 6400
  shape: [58436, 68, 32, 64]
  preload_to_memory: False
  augment: False
  train_range: [0,56000]
  val_range: [56000,58436]
  sample: 
    name: yl
    n: 10
    T_max: 200


finetune: 
  # original_dir: ../dataset
  # initialize_checkpoint: /mnt/vepfs/devel/weather/deepspeed_checkpoint/multivarible-swin-192-patch1-[1,1,6]-uncertainty-fusion-layer/step_86000/mp_rank_00_model_states.pt
  # initialize_checkpoint: /mnt/vepfs/devel/weather/deepspeed_checkpoint/singlevariable-384-patch1-meaninguncertainty-4-clipping/step_66000/mp_rank_00_model_states.pt
  initialize_checkpoint: /mnt/vepfs/devel/weather/deepspeed_checkpoint/singlevariable-384-patch1-meaninguncertainty-4-clipping/step_86000/mp_rank_00_model_states.pt
  # initialize_checkpoint: /mnt/vepfs/devel/weather/deepspeed_checkpoint/multivarible-swin-96-patch1-[1,1,6]-uncertainty-fusion-layer/step_86000/mp_rank_00_model_states.pt
  # initialize_checkpoint: /mnt/vepfs/devel/weather/finetune_checkpoint/singlevariable-384-patch1-meaninguncertainty-4-clipping_step_86000_yl/step_40000/mp_rank_00_model_states.pt
train:   
  optimizer:
    name: Adam
    learning_rate: 0.1  
    weight_decay: 1e-5
    scheduler: 
      name: NOPE
      T_max: 40000
      eta_min: 1.0e-9
      warmup_steps: 1000
  time_regularization: False
  n_epochs: 350
  ckpt_dir: /mnt/vepfs/devel/weather/finetune_checkpoint
  device: cuda:6
  uncertainty_loss: False
  validate_step: 2000
  uncertainty_finetune: False

  dataloader:
    num_workers: 16
    batch_size: 96
    prefetch_factor: 16
  # deepspeed_config: ./cfgs/deepspeed_finetune-cos-4e-6-nodecay.json
  deepspeed_config_yaml:
    train_batch_size: 32
    train_micro_batch_size_per_gpu: 32
    optimizer:
      type: Adam
      params:
        lr: 2.0e-6
        weight_decay: 0
    zero_optimization:
      stage: 0
    gradient_clipping: 5
    data_efficiency:
      data_sampling:
        enabled: true
        num_workers: 32
        prefetch_factor: 32


logger: #default wandb
   project: fengwu-finetune
   name: singlevariable-384-patch1-meaninguncertainty-4-clipping_step_86000_yl_2e-6-fix-uncertaintyfinetune

seed: 114514

