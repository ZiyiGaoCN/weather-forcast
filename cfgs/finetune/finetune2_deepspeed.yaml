model:
  model_path: model.swin_unet #'dense' or 'conv', 'dense2' or 'conv2'
  model_type: SwinTransformerSys
  param: 
    param: 
    in_seq_length: 2
    out_seq_length: 1
    input_dim: 70
    output_dim: 70
    lat_dim: 161
    lon_dim: 161 # now must be same as lat_dim    # above are input and output parameters
    patch_size: 2
    hidden_dim: 384
    window_size: 10
    ape: True
    # drop_rate : 0.5
    # attn_drop_rate : 0.5
    

data: 
  data_path : /dev/shm/store/checkpoint/original_dataset/ #the folder of 3 npy file
  # npy_name: ('data1.npy', 'data2.npy', 'data3.npy') # order: target_npy, input1.npy, input2.npy, if None, write (a.npy, None, None)
  # npy_len: (7304 , -1, -1) #3-tuple, set -1 if the npy not exist 
  # 14612 for 2007-2016
  npy_name: /dev/shm/store/original_dataset/dataset.npy
  shape: [58436, 73, 32, 64]
  preload_to_memory: False
  augment: False
  train_range: [0,56000]
  val_range: [56000,58436]

finetune: 
  # original_dir: ../dataset
  generated_dir: /dev/shm/store/checkpoint/dataset_2
  checkpoint_dir: /dev/shm/store/checkpoint/checkpoint_2

train:   
  optimizer:
    name: Adam
    learning_rate: 0.1  
    weight_decay: 1e-5
    scheduler: 
      name: NoamLR
      hidden_dim: 192
      warmup_steps: 100
  n_epochs: 6
  ckpt_dir: checkpoint
  device: cuda:6
  autoregressive: True

  dataloader:
    num_workers: 16
    batch_size: 96
    prefetch_factor: 16
  deepspeed_config: ./cfgs/deepspeed_finetune.json

logger: #default wandb
   project: weather-finetune
   name: swin-AR-finetune

seed: 114514

