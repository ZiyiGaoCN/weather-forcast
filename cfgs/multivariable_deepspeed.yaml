model:
  model_path: model.swin_unet_multivariable #'dense' or 'conv', 'dense2' or 'conv2'
  model_type: Model
  param: 
    lat_dim: 32
    lon_dim: 64 # above are input and output parameters
    hidden_dim: 512
    ape: True
    patch_size: [1,1]
    depths: [2]
    fusion_layers: [4,12]
    window_size: [4,8]
    fusion_window_size: [4,8,6]
    num_heads: [4,8]
    # fusion_hidden_size: [96,192,384]
    fusion_hidden_size: [512,512]
    # fusion_hidden_size: [768,1536,3072]
    time_embed: True
    variable_dict:
      temperature: 13
      v_component_of_wind: 13
      specific_humidity: 13
      geopotential: 13
      u_component_of_wind: 13
      surface_variable: 3
    variable_down: [68,43,28]
    variable_order:
      - temperature
      - v_component_of_wind
      - specific_humidity
      - geopotential
      - u_component_of_wind
      - surface_variable
    variable_fusion: False
    uncertainty_loss: True
    drop_rate: 0.1
    attn_drop_rate: 0.1


data: 
  data_path : /dev/shm/store/checkpoint/original_dataset
  npy_name: /dev/shm/store/original_dataset/dataset.npy
  shape: [58436, 68, 32, 64]
  preload_to_memory: False
  augment: False
  train_range: [0,56000]
  val_range: [56000,58436]
  input_step: 1

train: 
  optimizer:
    name: Adam
    learning_rate: 1
    weight_decay: 1e-5
    scheduler: 
      # name: NoamLR
      # hidden_dim: 192
      # warmup_steps: 1000
      name: CosineAnnealingLR
      T_max: 87500
      eta_min: 1.0e-8
      warmup_steps: 1000
  n_epochs: 50  
  ckpt_dir: /mnt/vepfs/devel/gaoziyi/weather/deepspeed_checkpoint
  device: cuda:0
  autoregressive: True
  validate_step : 2000
  validate_dataloader:
    batch_size: 32
    num_workers: 16
    prefetch_factor: 16
    shuffle: False


  uncertainty_loss: True
  time_regularization: False
  time_regularization_weight: 1
  meaning_uncertainty: False
  gradient_clip: 32

  batch_size: 64

  restore_session:
    yes: False
    load_dir: checkpoint
    ckpt_id: 200

  dataloader:
    num_workers: 32
    batch_size: 64
    prefetch_factor: 32

  deepspeed_config: ./cfgs/deepspeed.json
  deepspeed_config_yaml:
    train_batch_size: 32
    train_micro_batch_size_per_gpu: 4  
    optimizer:
      type: Adam
      params:
        lr: 1.0e-4
        weight_decay: 0
    zero_optimization:
      stage: 0
    gradient_clipping: 1
    data_efficiency:
      data_sampling:
        enabled: true
        num_workers: 16
        prefetch_factor: 8


logger: #default wandb
    project: weather-forecast-deepspeed
    name: multivariable-512

seed: 114514

hydra:
  run:
    dir: null

    

    